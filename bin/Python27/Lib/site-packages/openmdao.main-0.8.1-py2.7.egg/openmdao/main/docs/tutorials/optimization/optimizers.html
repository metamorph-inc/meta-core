
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Choosing an Optimizer &mdash; OpenMDAO Documentation</title>
    
    <link rel="stylesheet" href="../../_static/default.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     '0.8.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="OpenMDAO Documentation" href="../../index.html" />
    <link rel="up" title="Simple Optimization" href="index.html" />
    <link rel="next" title="MetaModel" href="../surrogate/index.html" />
    <link rel="prev" title="Adding Derivatives to Your Components" href="derivatives.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../surrogate/index.html" title="MetaModel"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="derivatives.html" title="Adding Derivatives to Your Components"
             accesskey="P">previous</a> |</li>
  <li><a href="http://openmdao.org">OpenMDAO Home</a> &raquo;</li>
  
        <li><a href="../../index.html">OpenMDAO Documentation v0.8.1</a> &raquo;</li>

          <li><a href="../index.html" >OpenMDAO Tutorials</a> &raquo;</li>
          <li><a href="index.html" accesskey="U">Simple Optimization</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="choosing-an-optimizer">
<span id="optimizers"></span><span id="index-0"></span><h1>Choosing an Optimizer<a class="headerlink" href="#choosing-an-optimizer" title="Permalink to this headline">¶</a></h1>
<p>When the initial versions of OpenMDAO were released, the standard library contained only
two optimizers: the gradient optimizer CONMIN, and the genetic optimizer Genetic.
OpenMDAO now includes several optimizers, provides access to a few more optimizers as plugins,
and will continue to benefit from community contributions. Let&#8217;s walk through how you can
use these different optimizers on the paraboloid example problem.</p>
<div class="section" id="openmdao-optimizers">
<h2>OpenMDAO Optimizers<a class="headerlink" href="#openmdao-optimizers" title="Permalink to this headline">¶</a></h2>
<p>The following tables summarizes the optimizers that are currently included in OpenMDAO.</p>
<table border="1" class="docutils">
<colgroup>
<col width="10%" />
<col width="13%" />
<col width="6%" />
<col width="6%" />
<col width="65%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Optimizer</th>
<th class="head">Gradients</th>
<th class="head">Inequality
Constraints</th>
<th class="head">Equality
Constraints</th>
<th class="head">Algorithm</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><tt class="docutils literal"><span class="pre">COBYLAdriver</span></tt></td>
<td>None</td>
<td>Yes</td>
<td>No</td>
<td>Constrained Optimization BY Linear Approximation of the objective and constraint functions via linear interpolation</td>
</tr>
<tr class="row-odd"><td><tt class="docutils literal"><span class="pre">CONMINdriver</span></tt></td>
<td>Computed by OpenMDAO
or CONMIN</td>
<td>Yes</td>
<td>No</td>
<td>CONstrained function MINimization. Implements the Method of Feasible Directions to solve the NLP problem</td>
</tr>
<tr class="row-even"><td><tt class="docutils literal"><span class="pre">Genetic</span></tt></td>
<td>None</td>
<td>No</td>
<td>No</td>
<td>General genetic algorithm framework based on PyEvolve</td>
</tr>
<tr class="row-odd"><td><tt class="docutils literal"><span class="pre">NEWSUMTdriver</span></tt></td>
<td>Computed by OpenMDAO
or NEWSUMT</td>
<td>Yes</td>
<td>No</td>
<td>NEWton&#8217;s method Sequence of Unconstrained Minimizations</td>
</tr>
<tr class="row-even"><td><tt class="docutils literal"><span class="pre">SLSQPdriver</span></tt></td>
<td>Computed by OpenMDAO</td>
<td>Yes</td>
<td>Yes</td>
<td>Sequential Least SQuares Programming</td>
</tr>
</tbody>
</table>
<p>Any of these optimizers can be added to your model by importing it from <tt class="docutils literal"><span class="pre">openmdao.lib.drivers.api</span></tt>. Which
optimizer should you use? The answer is highly problem dependent. For example, if your problem has equality
constraints, then you can use only the SLSQPdriver unless you can rewrite your equality constraint as a
pair of inequality constraints. Similarly, COBYLAdriver is a gradient-free optimization, which might be
suitable if you want to avoid finite difference calculations.</p>
<p>In the example below, you will have the opportunity to try out all of the OpenMDAO optimizers on the
Paraboloid problem. This will give you some basic insight into their performance and should familiarize
you with their most useful settings.</p>
</div>
<div class="section" id="swapping-optimizers">
<h2>Swapping Optimizers<a class="headerlink" href="#swapping-optimizers" title="Permalink to this headline">¶</a></h2>
<p>In previous examples, we found the minimum value of a paraboloid both with and without constraints. We also
learned how to add derivative functions to our components and use them in an optimizer&#8217;s calculation of the
gradient. We did all of this using the CONMIN optimizer. Now, let&#8217;s investigate the rest of OpenMDAO&#8217;s
optimizers and see how they compare to CONMIN.</p>
<p>Let&#8217;s start by creating the following model and calling it <tt class="docutils literal"><span class="pre">demo_opt.py</span></tt>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="sd">&#39;&#39;&#39; Demonstration of swapping optimizers on a problem &#39;&#39;&#39;</span>

<span class="kn">from</span> <span class="nn">openmdao.examples.simple.paraboloid_derivative</span> <span class="kn">import</span> <span class="n">ParaboloidDerivative</span>
<span class="kn">from</span> <span class="nn">openmdao.lib.differentiators.api</span> <span class="kn">import</span> <span class="n">FiniteDifference</span>
<span class="kn">from</span> <span class="nn">openmdao.lib.drivers.api</span> <span class="kn">import</span> <span class="n">COBYLAdriver</span><span class="p">,</span> <span class="n">CONMINdriver</span><span class="p">,</span> \
        <span class="n">NEWSUMTdriver</span><span class="p">,</span> <span class="n">SLSQPdriver</span><span class="p">,</span> <span class="n">Genetic</span>
<span class="kn">from</span> <span class="nn">openmdao.main.api</span> <span class="kn">import</span> <span class="n">Assembly</span>

<span class="k">class</span> <span class="nc">DemoOpt</span><span class="p">(</span><span class="n">Assembly</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constrained optimization of the Paraboloid with whatever optimizer</span>
<span class="sd">    we want.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">configure</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Creates a new Assembly containing a Paraboloid and an optimizer&quot;&quot;&quot;</span>

        <span class="c"># Create Paraboloid component instances</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s">&#39;comp&#39;</span><span class="p">,</span> <span class="n">ParaboloidDerivative</span><span class="p">())</span>

        <span class="c"># Create Optimizer instance</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s">&#39;driver&#39;</span><span class="p">,</span> <span class="n">CONMINdriver</span><span class="p">())</span>

        <span class="c"># Driver process definition</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">workflow</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s">&#39;comp&#39;</span><span class="p">)</span>

        <span class="c"># Objective</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">add_objective</span><span class="p">(</span><span class="s">&#39;comp.f_xy&#39;</span><span class="p">)</span>

        <span class="c"># Design Variables</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">add_parameter</span><span class="p">(</span><span class="s">&#39;comp.x&#39;</span><span class="p">,</span> <span class="n">low</span><span class="o">=-</span><span class="mf">50.</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mf">50.</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">add_parameter</span><span class="p">(</span><span class="s">&#39;comp.y&#39;</span><span class="p">,</span> <span class="n">low</span><span class="o">=-</span><span class="mf">50.</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mf">50.</span><span class="p">)</span>

        <span class="c"># Inequality Constraints</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">add_constraint</span><span class="p">(</span><span class="s">&#39;comp.x-comp.y &gt;= 15.0&#39;</span><span class="p">)</span>

        <span class="c"># Equality Constraints</span>
        <span class="c">#self.driver.add_constraint(&#39;comp.x-comp.y=15.0&#39;)</span>

        <span class="c"># Differentiator</span>
        <span class="c">#self.driver.differentiator = FiniteDifference()</span>

        <span class="c"># General flag - suppress output</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">iprint</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c"># CONMIN-specific Settings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">itmax</span> <span class="o">=</span> <span class="mi">30</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">fdch</span> <span class="o">=</span> <span class="mf">0.00001</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">fdchm</span> <span class="o">=</span> <span class="mf">0.000001</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">ctlmin</span> <span class="o">=</span> <span class="mf">0.01</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">delfun</span> <span class="o">=</span> <span class="mf">0.001</span>

        <span class="c"># NEWSUMT-specific Settings</span>
        <span class="c">#self.driver.itmax = 10</span>

        <span class="c"># COBYLA-specific Settings</span>
        <span class="c">#self.driver.rhobeg = 1.0</span>
        <span class="c">#self.driver.rhoend = 1.0e-4</span>
        <span class="c">#self.driver.maxfun = 1000</span>

        <span class="c"># SLSQP-specific Settings</span>
        <span class="c">#self.driver.accuracy = 1.0e-6</span>
        <span class="c">#self.driver.maxiter = 50</span>

        <span class="c"># Genetic-specific Settings</span>
        <span class="c">#self.driver.population_size = 90</span>
        <span class="c">#self.driver.crossover_rate = 0.9</span>
        <span class="c">#self.driver.mutation_rate = 0.02</span>
        <span class="c">#self.selection_method = &#39;rank&#39;</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">&quot;__main__&quot;</span><span class="p">:</span> <span class="c"># pragma: no cover</span>

    <span class="kn">import</span> <span class="nn">time</span>

    <span class="n">opt_problem</span> <span class="o">=</span> <span class="n">DemoOpt</span><span class="p">()</span>

    <span class="n">t1</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">opt_problem</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">t2</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">print</span> <span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span>
    <span class="k">print</span> <span class="s">&quot;Optimizer: </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">opt_problem</span><span class="o">.</span><span class="n">driver</span><span class="p">)</span>
    <span class="k">print</span> <span class="s">&quot;Function executions: &quot;</span><span class="p">,</span> <span class="n">opt_problem</span><span class="o">.</span><span class="n">comp</span><span class="o">.</span><span class="n">exec_count</span>
    <span class="k">print</span> <span class="s">&quot;Gradient executions: &quot;</span><span class="p">,</span> <span class="n">opt_problem</span><span class="o">.</span><span class="n">comp</span><span class="o">.</span><span class="n">derivative_exec_count</span>
    <span class="k">print</span> <span class="s">&quot;Minimum: </span><span class="si">%f</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">opt_problem</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">eval_objective</span><span class="p">()</span>
    <span class="k">print</span> <span class="s">&quot;Minimum found at (</span><span class="si">%f</span><span class="s">, </span><span class="si">%f</span><span class="s">)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">opt_problem</span><span class="o">.</span><span class="n">comp</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> \
                                         <span class="n">opt_problem</span><span class="o">.</span><span class="n">comp</span><span class="o">.</span><span class="n">y</span><span class="p">)</span>
    <span class="k">print</span> <span class="s">&quot;Elapsed time: &quot;</span><span class="p">,</span> <span class="n">t2</span><span class="o">-</span><span class="n">t1</span><span class="p">,</span> <span class="s">&quot;seconds&quot;</span>
</pre></div>
</div>
<p>We&#8217;ve gone ahead and imported every optimizer to make swapping them fairly easy. Several
blocks of lines are commented out in this code. Most of these contain settings for the optimizers.
Settings are usually very specific to an optimizer, so we&#8217;ll want to take care that only the lines for
the optimizer we are using are active. The parameters, objective(s), and constraints(s) can all stay the
same when you swap in a new optimizer, provided they are supported (e.g., equality constraints are only
supported by SLSQPdriver.) Also, we will sometimes slot a FiniteDifference differentiator, though that
line of code is currently commented out. Some optimizers, like CONMINdriver, have their own finite
difference capability. Others, like <tt class="docutils literal"><span class="pre">SLSQ_driver</span></tt>, do not and use the one from OpenMDAO. Regardless,
since we&#8217;re using the ParaboloidDerivative component, which contains the analytical derivatives,
all of the finite difference calculations will use the FDAD (Finite Difference with Analytical
Derivatives) approach. So even if the optimizer is trying to do finite differences, OpenMDAO will
use the analytic derivatives that are provided to speed up the optimization.</p>
<p>So first, let&#8217;s run <a class="reference download internal" href="../../_downloads/demo_opt.py"><tt class="xref download docutils literal"><span class="pre">demo_opt.py</span></tt></a>.  This first case is the
constrained optimization of the paraboloid using CONMIN&#8217;s internal finite difference calculation.</p>
<p>Note that the sample results presented here are representative of what you should see, but they
may differ depending on your system architecture.</p>
<div class="highlight-python"><pre>Optimizer: &lt;class 'openmdao.lib.drivers.conmindriver.CONMINdriver'&gt;
Function executions:  16
Gradient executions:  6
Minimum: -27.083084
Minimum found at (7.175777, -7.824223)
Elapsed time:  0.0239610671997 seconds</pre>
</div>
<p>We obtained this value after adjusting some of CONMIN&#8217;s settings from their defaults.
CONMIN is notoriously sensitive to the values of these settings, in particular the
relative and minimum absolute stepsize changes in the finite difference calculation (<cite>fdch</cite> and
<cite>fdchm</cite>). The answer that CONMIN gives here didn&#8217;t quite reach the minimum, which we&#8217;ve found with
other optimizers to lie at <tt class="docutils literal"><span class="pre">(7.166667,</span> <span class="pre">-7.833333)</span></tt>. Exploring CONMIN&#8217;s settings could
possibly yield a better answer, but that&#8217;s not a reasonable thing to do for a real problem.</p>
<p>Next we&#8217;ll let OpenMDAO perform the finite difference instead of CONMIN. To do this, uncomment the
line that sockets the differentiator.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># Differentiator</span>
<span class="bp">self</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">differentiator</span> <span class="o">=</span> <span class="n">FiniteDifference</span><span class="p">()</span>
</pre></div>
</div>
<p>Then run <tt class="docutils literal"><span class="pre">demo_opt.py</span></tt>.</p>
<div class="highlight-python"><pre>Function executions:  18
Gradient executions:  5
Minimum: -27.075841
Minimum found at (7.200896, -7.808874)
Elapsed time:  0.0260651111603 seconds</pre>
</div>
<p>The answer here is actually a little worse than before. There are a couple of possible reasons for
this. OpenMDAO&#8217;s finite difference is fairly simple, with a single non-adapting stepsize. This
stepsize could be specified for each parameter, though the scaling for <cite>x</cite> and <cite>y</cite> here is
roughly the same, so it wouldn&#8217;t be needed. On the other hand, CONMIN uses an adaptive stepsize
which presumably takes smaller steps as it approaches the optimum, so this should do a better
job. Moreover, some time was spent picking a reasonable stepsize for CONMIN, but for the
OpenMDAO differentiator, we just kept the default value.</p>
<p>Now, let&#8217;s try the NEWSUMT driver. First, replace <tt class="docutils literal"><span class="pre">CONMINdriver</span></tt> with <tt class="docutils literal"><span class="pre">NEWSUMTdriver</span></tt>
where it is added to the assembly.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># Create Optimizer instance</span>
<span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s">&#39;driver&#39;</span><span class="p">,</span> <span class="n">NEWSUMTdriver</span><span class="p">())</span>
</pre></div>
</div>
<p>We need to use the NEWSUMT settings and deactivate the CONMIN settings.
Let&#8217;s also unsocket OpenMDAO&#8217;s finite difference.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># Differentiator</span>
<span class="c">#self.driver.differentiator = FiniteDifference()</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># CONMIN-specific Settings</span>
<span class="c">#self.driver.itmax = 30</span>
<span class="c">#self.driver.fdch = 0.00001</span>
<span class="c">#self.driver.fdchm = 0.000001</span>
<span class="c">#self.driver.ctlmin = 0.01</span>
<span class="c">#self.driver.delfun = 0.001</span>

<span class="c"># NEWSUMT-specific Settings</span>
<span class="bp">self</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">itmax</span> <span class="o">=</span> <span class="mi">10</span>
</pre></div>
</div>
<p>Then run <tt class="docutils literal"><span class="pre">demo_opt.py</span></tt>.</p>
<div class="highlight-python"><pre>Optimizer: &lt;class 'openmdao.lib.drivers.newsumtdriver.NEWSUMTdriver'&gt;
Function executions:  126
Gradient executions:  10
Minimum: -25.785512
Minimum found at (7.910433, -8.577796)
Elapsed time:  0.0497758388519 seconds</pre>
</div>
<p>We didn&#8217;t do as well here with NEWSUMT. However, the default number of iterations for NEWSUMT is 10. We can
tell that we&#8217;re hitting this number because we&#8217;ve performed 10 gradient executions. We could also tell this
from the number of driver iterations, which in NEWSUMT is stored in <tt class="docutils literal"><span class="pre">iter_count</span></tt>. Note that not every
driver reports an iteration count, so we didn&#8217;t print it here. Let&#8217;s boost our maximum number of iterations:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># NEWSUMT-specific Settings</span>
<span class="bp">self</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">itmax</span> <span class="o">=</span> <span class="mi">50</span>
</pre></div>
</div>
<p>Then run <tt class="docutils literal"><span class="pre">demo_opt.py</span></tt>.</p>
<div class="highlight-python"><pre>Optimizer: &lt;class 'openmdao.lib.drivers.newsumtdriver.NEWSUMTdriver'&gt;
Function executions:  253
Gradient executions:  26
Minimum: -27.079630
Minimum found at (7.170354, -7.837026)
Elapsed time:  0.107419013977 seconds</pre>
</div>
<p>Our answer has improved and is slightly better than what CONMIN reported. Notice that the
number of functional executions is an order of magnitude more than CONMIN. For a problem
with a long runtime, this optimizer may be significantly slower.</p>
<p>Next, let&#8217;s slot OpenMDAO&#8217;s finite difference differentiator.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># Differentiator</span>
<span class="bp">self</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">differentiator</span> <span class="o">=</span> <span class="n">FiniteDifference</span><span class="p">()</span>
</pre></div>
</div>
<p>Then run <tt class="docutils literal"><span class="pre">demo_opt.py</span></tt>.</p>
<div class="highlight-python"><pre>Optimizer: &lt;class 'openmdao.lib.drivers.newsumtdriver.NEWSUMTdriver'&gt;
Function executions:  255
Gradient executions:  50
Minimum: -27.079630
Minimum found at (7.170357, -7.837023)
Elapsed time:  0.133186101913 seconds</pre>
</div>
<p>The answer is about the same. One notable difference is a doubling of the number of gradient executions.
This is because NEWSUMT is the only optimizer which asks for an explicit Hessian (i.e., 2nd derivative)
of the objective and constraints. Hessian calculation is expensive and scales n-squared with the number
of parameters. When NEWSUMT calculates the Hessian internally, it&#8217;s using some approximations to speed
the calculation. Thus, it might be advisable to use NEWSUMT&#8217;s gradient calculation.</p>
<p>Now let&#8217;s try the COBYLAdriver.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># Create Optimizer instance</span>
<span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s">&#39;driver&#39;</span><span class="p">,</span> <span class="n">COBYLAdriver</span><span class="p">())</span>
</pre></div>
</div>
<p>We don&#8217;t have to unsocket the finite difference driver, as COBYLA is a gradient-free method and
will not use it. But you can comment it out if you want to; the answer won&#8217;t change.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># COBYLA-specific Settings</span>
<span class="bp">self</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">rhobeg</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="bp">self</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">rhoend</span> <span class="o">=</span> <span class="mf">1.0e-4</span>
<span class="bp">self</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">maxfun</span> <span class="o">=</span> <span class="mi">1000</span>
</pre></div>
</div>
<p>COBYLA has very few settings. The <tt class="docutils literal"><span class="pre">rhoend</span></tt> parameter is equivalent to a convergence tolerance, and
<tt class="docutils literal"><span class="pre">maxfun</span></tt> is the maximum number of iterations. Now try running <tt class="docutils literal"><span class="pre">demo_opt.py</span></tt>.</p>
<div class="highlight-python"><pre>Optimizer: &lt;class 'openmdao.lib.drivers.cobyladriver.COBYLAdriver'&gt;
Function executions:  47
Gradient executions:  0
Minimum: -27.083333
Minimum found at (7.166766, -7.833234)
Elapsed time:  0.0164699554443 seconds</pre>
</div>
<p>The answer is considerably better than CONMIN. Let&#8217;s experiment with the convergence criterion
by decreasing <tt class="docutils literal"><span class="pre">rhoend</span></tt> to <tt class="docutils literal"><span class="pre">1.0e-5</span></tt>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># COBYLA-specific Settings</span>
<span class="bp">self</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">rhobeg</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="bp">self</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">rhoend</span> <span class="o">=</span> <span class="mf">1.0e-5</span>
<span class="bp">self</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">maxfun</span> <span class="o">=</span> <span class="mi">1000</span>
</pre></div>
</div>
<p>Run <tt class="docutils literal"><span class="pre">demo_opt.py</span></tt>:</p>
<div class="highlight-python"><pre>Optimizer: &lt;class 'openmdao.lib.drivers.cobyladriver.COBYLAdriver'&gt;
Function executions:  54
Gradient executions:  0
Minimum: -27.083333
Minimum found at (7.166661, -7.833339)
Elapsed time:  0.0184278488159 seconds</pre>
</div>
<p>This results in seven more function executions and a better minimum (although the value of the minimum is cut
off in our printout because of the print display resolution &#8211; you can make it more explicit with a
specified-width format, like <tt class="docutils literal"><span class="pre">%.15f</span></tt>). COBYLA needed three times the number of function evaluations as
CONMIN, but it got to a much better value, and it does not exhibit any hyper-sensitivity with respect to its
settings. Note also that COBYLA&#8217;s elapsed time is still lower. The optimizer seems to have less overhead,
which affects the total wall time for trivial functions like our paraboloid. But that overhead won&#8217;t matter
for real analyses that have any appreciable computational cost.</p>
<p>Next up is SLSQP. This optimizer requires a gradient but has no internal finite difference calculations,
so by default SLSQPdriver always uses the OpenMDAO finite difference engine. Add an SLSQPdriver
instance to your model:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># Create Optimizer instance</span>
<span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s">&#39;driver&#39;</span><span class="p">,</span> <span class="n">SLSQPdriver</span><span class="p">())</span>
</pre></div>
</div>
<p>SLSQP only has a couple of settings, none of which will be moved off the default.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># SLSQP-specific Settings</span>
<span class="bp">self</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">accuracy</span> <span class="o">=</span> <span class="mf">1.0e-6</span>
<span class="bp">self</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">maxiter</span> <span class="o">=</span> <span class="mi">50</span>
</pre></div>
</div>
<p>Now, let&#8217;s run <tt class="docutils literal"><span class="pre">demo_opt.py</span></tt>:</p>
<div class="highlight-python"><pre>Optimizer: &lt;class 'openmdao.lib.drivers.slsqpdriver.SLSQPdriver'&gt;
Function executions:  4
Gradient executions:  3
Minimum: -27.083333
Minimum found at (7.166667, -7.833334)
Elapsed time:  0.00905513763428 seconds</pre>
</div>
<p>The SLSQP driver performs incredibly well on this problem! It gets the closest to the minimum with the least
number of function executions and in the quickest wall time. It&#8217;s also our only optimizer that can directly
handle equality constraints, so let&#8217;s try one. We already know that the solution to our constrained problem
lies along the constraint. We could express this as an equality constraint and expect that the same solution
would be reached. The equality constraint was included in <tt class="docutils literal"><span class="pre">demo_opt.py</span></tt>, so comment and uncomment as such:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># Inequality Constraints</span>
<span class="c">#self.driver.add_constraint(&#39;comp.x-comp.y &gt;= 15.0&#39;)</span>

<span class="c"># Equality Constraints</span>
<span class="bp">self</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">add_constraint</span><span class="p">(</span><span class="s">&#39;comp.x-comp.y=15.0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Equality constraints are constructed as expression strings just like inequality constraints. Now
let&#8217;s run <tt class="docutils literal"><span class="pre">demo_opt.py</span></tt>:</p>
<div class="highlight-python"><pre>Optimizer: &lt;class 'openmdao.lib.drivers.slsqpdriver.SLSQPdriver'&gt;
Function executions:  4
Gradient executions:  3
Minimum: -27.083333
Minimum found at (7.166667, -7.833334)
Elapsed time:  0.00990891456604 seconds</pre>
</div>
<p>We arrive at the same answer with the equality constraint.</p>
<p>And now for something completely different, let&#8217;s try the Genetic optimizer.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># Create Optimizer instance</span>
<span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s">&#39;driver&#39;</span><span class="p">,</span> <span class="n">Genetic</span><span class="p">())</span>
</pre></div>
</div>
<p>Genetic is currently our only evolutionary algorithm optimizer. As such, it has some
settings that are quite different:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># SLSQP-specific Settings</span>
<span class="c">#self.driver.accuracy = 1.0e-6</span>
<span class="c">#self.driver.maxiter = 50</span>

<span class="c"># Genetic-specific Settings</span>
<span class="bp">self</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">population_size</span> <span class="o">=</span> <span class="mi">90</span>
<span class="bp">self</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">crossover_rate</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="bp">self</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">mutation_rate</span> <span class="o">=</span> <span class="mf">0.02</span>
<span class="bp">self</span><span class="o">.</span><span class="n">selection_method</span> <span class="o">=</span> <span class="s">&#39;rank&#39;</span>
</pre></div>
</div>
<p>These are mostly the default values, although <tt class="docutils literal"><span class="pre">selection_method</span></tt> was changed to <tt class="docutils literal"><span class="pre">'rank'</span></tt> because
it seemed to give better answers for this problem. Genetic doesn&#8217;t use any gradient
information, so we don&#8217;t need to worry about finite difference calculations here. Also, Genetic doesn&#8217;t handle any kind of
constraints, so we&#8217;ll only be able to play around with the unconstrained problem.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># Inequality Constraints</span>
<span class="c">#self.driver.add_constraint(&#39;comp.x-comp.y &gt;= 15.0&#39;)</span>

<span class="c"># Equality Constraints</span>
<span class="c">#self.driver.add_constraint(&#39;comp.x-comp.y=15.0&#39;)</span>
</pre></div>
</div>
<p>Now we are ready to run <tt class="docutils literal"><span class="pre">demo_opt.py</span></tt>:</p>
<div class="highlight-python"><pre>Optimizer: &lt;class 'openmdao.lib.drivers.genetic.Genetic'&gt;
Function executions:  8072
Gradient executions:  0
Minimum: -23.461808
Minimum found at (8.805645, -9.066226)
Elapsed time:  2.13916110992 seconds</pre>
</div>
<p>There should be no surprises here. This is not the kind of problem you would normally throw at
a genetic algorithm. Note that the answers are not deterministic, so re-running this will always give
different results.</p>
</div>
<div class="section" id="optimizers-from-plugins">
<h2>Optimizers from Plugins<a class="headerlink" href="#optimizers-from-plugins" title="Permalink to this headline">¶</a></h2>
<p>If you would like to choose from even more optimizers, look at the official plugins repository. This
repository generally contains OpenMDAO plugins that are wrappers of other existing external applications which
could not be included in OpenMDAO. Some of these may be commercial products (like Nastran), but others may be
open source packages. Most of  the time, the plugin contains just the OpenMDAO wrapper file, and you will need
to procure and install the application on its own. Presently, the official plugins repository contains two
optimizers.  The <tt class="docutils literal"><span class="pre">ipopt_wrapper</span></tt> optimizer is a wrapper for the <a class="reference external" href="https://projects.coin-or.org/Ipopt">IPOPT</a>  interior point optimizer, while <tt class="docutils literal"><span class="pre">pyopt_driver</span></tt> is a wrapper for the
<a class="reference external" href="http://www.pyopt.org/">pyOpt</a> optimization framework. You should definitely check out pyOpt because it
contains more than 15 optimization algorithms, most of which aren&#8217;t in OpenMDAO. Roughly half of them are
included in the pyOpt installation, while the other half are commercial and require a separate installation of
the optimization code. Some of the pyOpt&#8217;s optimizers include ALPSO (Augmented Lagrangian Particle Swarm
Optimizer), SNOPT (Sparse NOnlinear OPTimizer), and the famous NSGA2 (Non Sorting Genetic Algorithm II). To
install the <tt class="docutils literal"><span class="pre">pyopt_driver</span></tt>, type the following in an activated OpenMDAO environment at your operating system
prompt:</p>
<div class="highlight-python"><pre>plugin install --github pyopt_driver</pre>
</div>
<p>Note that you will also need to install pyOpt separately, either into your system environment or
directly into OpenMDAO&#8217;s Python.</p>
<p>This concludes the tutorial on optimizers.</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
<a href="http://openmdao.org">
 <img src="../../_static/OpenMDAO_Logo_200w_padded.png"
border="0" alt="OpenMDAO Home"/>
</a>

  <h3><a href="../../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Choosing an Optimizer</a><ul>
<li><a class="reference internal" href="#openmdao-optimizers">OpenMDAO Optimizers</a></li>
<li><a class="reference internal" href="#swapping-optimizers">Swapping Optimizers</a></li>
<li><a class="reference internal" href="#optimizers-from-plugins">Optimizers from Plugins</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="derivatives.html"
                        title="previous chapter">Adding Derivatives to Your Components</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="../surrogate/index.html"
                        title="next chapter">MetaModel</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="../../_sources/tutorials/optimization/optimizers.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../surrogate/index.html" title="MetaModel"
             >next</a> |</li>
        <li class="right" >
          <a href="derivatives.html" title="Adding Derivatives to Your Components"
             >previous</a> |</li>
  <li><a href="http://openmdao.org">OpenMDAO Home</a> &raquo;</li>
  
        <li><a href="../../index.html">OpenMDAO Documentation v0.8.1</a> &raquo;</li>

          <li><a href="../index.html" >OpenMDAO Tutorials</a> &raquo;</li>
          <li><a href="index.html" >Simple Optimization</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright none.
      Last updated on Aug 26, 2013.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.3.
    </div>
  </body>
</html>